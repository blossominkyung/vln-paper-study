## vln-paper-study
This repository is to review vision-language-navigation papers for personal study.  
However, sometimes I also review multimodal-learning, graphml, and other deep learnings. 

### contents
All review contents are in [Issues](https://github.com/blossominkyung/vln-paper-study/issues), also in [personal blog](https://www.blossominkyung.com/deeplearning) but by Korean.  
If you have any interest, then please check Issues in this repository.

- [X] FIBER: Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone(Microsoft, NeurIPS 2022) [click here!](https://github.com/blossominkyung/vln-paper-study/issues/3).

- [X] Teaching old labels in Heterogeneous Graphs via Knowledge Transfer Networks(Google Research, NeurIPS 2022) [click here!](https://github.com/blossominkyung/vln-paper-study/issues/4). 

- [X] History Aware Multimodal Transformer for Vision-and-Language Navigation(NeurIPS 2021) [click here!](https://github.com/blossominkyung/vln-paper-study/issues/2)

- [ ] TEACh: Task-driven Embodied Agents that Chat(Amazon Alexa AI, AAAI 2022). 
- [ ] Think Global, Act Local(CVPR 2022). 
- [ ] LERF: Language Embedded Radiance Fields(UC Berkely, 2023). 
- [ ] Generative Agents: Interactive Simulacra of Human Behavior(Standford&Google Research, 2023). 
- [ ] LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action(CoRL 2022). 
- [ ] Iterative Vision-and-Language Navigation(CVPR 2023). 
- [ ] KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation(CVPR 2023). 
- [ ] Visual Language Maps for Robot Navigation(arXiv 2022). 
- [ ] Multimodal Transformer with Variable-length Memory for Vision-and-Language Navigation(ECCV 2022). 
